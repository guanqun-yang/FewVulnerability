import os
import re
import sys
import ast
import pickle
import shutil
import logging
import pathlib
import hashlib
import itertools

import numpy as np
import pandas as pd

from datetime import datetime
from collections import defaultdict
from sklearn.metrics import classification_report

from setting import setting

def get_current_time():
    return datetime.now().strftime("%Y-%m-%d_%H:%M:%S")


def load_text_file(filename):
    if not pathlib.Path(filename).exists(): 
        print("not existent")
        return None

    with open(filename, "r") as fp:
        return fp.readlines()


def convert_text_to_shake(text):
    return hashlib.shake_128(bytes(text, encoding="raw_unicode_escape")).hexdigest(5)


def print_tuple(tuple_list, max_char_length=50):
    # extended version of pring_triplet() that accepts tuple of any length
    # tuple_list: [(a1, b1, c1, d1,...), (a2, b2, c2, d2,...), ...]

    # if any of the tuple has more than n_token tokens, ignore extra tokens
    n_token = min(map(len, tuple_list))
    token_list_dict = defaultdict(list)

    length = 0
    full_string = ""
    string_format = ""
    for tup in tuple_list:    
        # length    
        max_len = max(map(len, tup))
        length += max_len

        # print format
        string_format += "{:<%d" % (max_len + 2) + "}"

        for i in range(n_token): token_list_dict[i].append(tup[i])

        if length >= max_char_length:
            # append
            for token_list in token_list_dict.values():
                full_string += "%s\n" % string_format.format(*token_list)
            full_string += "\n"

            # reset
            length = 0
            string_format = ""
            token_list_dict = defaultdict(list)
    
    # when remaining tokens is shorter than max_char_length, append remaining tokens
    for token_list in token_list_dict.values():
        full_string += "%s\n" % string_format.format(*token_list)

    print(full_string)
    

def compute_weighted_f1(metric, target_names=None):
    # metric: sklearn.metrics.classification_report
    assert set(target_names).issubset(set(metric.keys()))

    supports = np.array([metric[name]["support"] for name in target_names])
    weights = supports / np.sum(supports)

    weighted_f1 = np.dot(weights, np.array([metric[name]["f1-score"] for name in target_names]))

    return weighted_f1


def evaluate_prediction(y_true, y_pred):
    label_list = setting.label_list

    y_pred_list = list(itertools.chain(*y_pred))
    y_true_list = list(itertools.chain(*y_true))

    metric = classification_report(y_true=y_true_list, y_pred=y_pred_list, target_names=label_list, output_dict=True)
    metric_str = classification_report(y_true=y_true_list, y_pred=y_pred_list, target_names=label_list, output_dict=False)

    return metric, metric_str


def set_pandas_display():
    pd.options.display.max_columns = None
    pd.options.display.max_rows = None
    pd.options.display.max_colwidth = 80
    pd.options.display.expand_frame_repr = True


def get_single_ner(category, train=True, test=False):
    base_path = setting.data_base_path / pathlib.Path("ner_data")
    def parse_line(line):
        result = re.split(r"\s+", line)
        # record of interest
        token, label = result[0], result[1]

        return token, label
    
    filename = None

    if train and (not test): filename = base_path / pathlib.Path("%s_train.txt" % category)
    if (not train) and test: filename = base_path / pathlib.Path("%s_test.txt" % category)
    if (not train) and (not test): filename = base_path / pathlib.Path("%s_valid.txt" % category)

    
    if filename == None: return None
    if isinstance(filename, pathlib.PosixPath):
        if not filename.exists(): return None
    
    block_list = list()
    block_string = ""
    for line in load_text_file(filename):
        # equivalent to block_string = block_string + line (rather than block_string = line + block_string)
        block_string += line
        # when encountering empty line, append block_string to block_list, and empty block_string
        if not line.split():
            block_list.append((category, block_string))
            block_string = ""
            continue
    
    record_list = list()
    for category, block_string in block_list:
        record_dict = {"category": None, "sentence": "", "label": "", "tuple": list()}
        for line in block_string.split("\n"):
            if not line.strip("\n"): continue
            token, label = parse_line(line)
        
            record_dict["sentence"] += token + " "
            record_dict["label"] += label + " "
            record_dict["tuple"].append((token, label))
        
        record_dict["category"] = category
        record_list.append(record_dict)
    

    return pd.DataFrame(record_list)


def remove_directories(path_list):
    # path_list: a list of paths in the current directory
    for path in path_list:
        if os.path.exists(path): shutil.rmtree(path)


def prepare_logger(filename=None, name=None):
    name = __name__ if (name is None) or (not isinstance(name, str)) else name 

    logger = logging.getLogger(name)
    if filename is None: filename = get_current_time()
    
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s\n%(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler(filename)],
    )
    logger.setLevel(logging.INFO)

    return logger


def get_line_cnt(path):
    cnt = None
    with open(path, "r") as fp:
        cnt = len(fp.readlines())
    return cnt

