import os
import re
import time
import random
import hashlib
import pathlib
import numpy as np
import pandas as pd

from utils.common import get_single_ner
from utils.common import load_text_file
from utils.common import convert_text_to_shake


class SimpleNERDataset:
    def __init__(self, category="memc", train_frac=0.1, train_size=32, 
                       info_frac=None, train_valid_same_size=True, random_state=42):
        self.category = category

        # handling train_frac and train_size
        if (not train_frac) and (not train_size):
            raise ValueError("both train_frac and train_size are None")

        self.train_frac = train_frac
        self.train_size = train_size
        # if both train_frac and train_size are specified, then train_frac takes effect
        if train_frac and train_size: self.train_size = None

        # info_frac
        # - None: do not control the proportion of SN/SV and O sentence
        # - (0, 1]: the proportion of SN/SV sentence
        self.info_frac = info_frac
        # same_train_valid == True when enforcing same train and valid size when possible
        self.train_valid_same_size = train_valid_same_size
        # random state
        self.random_state = random_state

        # load data
        self.df = self.load_data()

        self.train_idx = self.get_index_from_name("train")
        self.pool_idx = self.get_index_from_name("pool")
        self.eval_idx = self.get_index_from_name("eval")
        self.test_idx = self.get_index_from_name("test")

    def load_data(self):
        # set seed
        random.seed(self.random_state)
        np.random.seed(self.random_state)

        # data processing
        df_dict = {"train": get_single_ner(self.category, train=True, test=False),
                   "valid": get_single_ner(self.category, train=False, test=False),
                   "test": get_single_ner(self.category, train=False, test=True)}

        # leave 10% of the df_dict["train"] as validation data when df_dict["valid"] is None
        if df_dict["valid"] is None:
            df_dict["valid"] = df_dict["train"].sample(frac=0.1, random_state=self.random_state)
            df_dict["train"] = df_dict["train"].drop(df_dict["valid"].index).reset_index(drop=True)
            df_dict["valid"] = df_dict["valid"].reset_index(drop=True)

        # drop duplicates
        for split in ["train", "valid", "test"]:
            df = df_dict[split]

            # drop duplicates
            df = df[~df.astype(str).duplicated()].reset_index(drop=True)

            # assign ID to each sentence
            df["ID"] = df["sentence"].apply(convert_text_to_shake)

            # test uniqueness
            assert df["ID"].nunique() == len(df)

            df_dict[split] = df
        

        for split in ["train", "valid", "test"]:
            # add train/valid/test label
            length = len(df_dict[split])
            df_dict[split]["split"] = [split for _ in range(length)]

            # determine whether the sentence is O only
            df_dict[split]["not_o_only"] = df_dict[split].tuple.apply(lambda x: ("SN" or "SV") in [tup[1] for tup in x])
        
        train_idx = self.get_train_idx(df_dict["train"])
        pool_idx = df_dict["train"].index.difference(train_idx)

        # train and pool
        df_dict["train"].loc[train_idx, "name"] = ["train" for _ in range(len(train_idx))]
        df_dict["train"].loc[pool_idx, "name"] = ["pool" for _ in range(len(pool_idx))]

        # valid and pool
        # when it is possible, make sure valid_size == train_size, otherwise do nothing
        train_size = len(train_idx)
        valid_size = len(df_dict["valid"])

        if self.train_valid_same_size and (valid_size > train_size): 
            df_dict["valid"]["name"] = random.sample(["eval"] * train_size + ["pool"] * (valid_size - train_size),
                                                      k=valid_size)
        else:
            df_dict["valid"]["name"] = ["eval" for _ in range(valid_size)]

        # test, after this, the test set will be left untouched until for the final model
        df_dict["test"]["name"] = ["test" for _ in range(len(df_dict["test"]))]

        # merge everything back
        df = pd.concat([df_dict[split] for split in ["train", "valid", "test"]]).reset_index(drop=True)

        return df

    def get_train_idx(self, df):
        # input: df_dict["train"]
        total_train_size = len(df)

        # sanity check
        if self.train_frac is None:
            if not (1 <= self.train_size <= total_train_size):
                raise ValueError(f"invalid train sample size: {self.train_size}")
        else:
            if not (0 < self.train_frac <= 1):
                raise ValueError(f"invalid train fraction: {self.train_frac}")

        # valid_train_size: number of sentences that do not just have O
        valid_train_size = df["not_o_only"].value_counts()[True]
        invalid_train_size = df["not_o_only"].value_counts()[False]

        # using either train_frac or train_size, use size to sample data
        size = None

        if self.train_frac is None: size = self.train_size
        else: size = int(self.train_frac * total_train_size)

        if self.info_frac is not None:
            n_valid = int(size * self.info_frac)
            n_invalid = int(size * (1 - self.info_frac))

            # whether valid and invalid samples are enough to sample
            if (n_valid <= valid_train_size) and (n_invalid <= invalid_train_size):
                valid_idx = df[df.not_o_only].sample(n=n_valid, replace=False, random_state=self.random_state).index.tolist()
                invalid_idx = df[~df.not_o_only].sample(n=n_invalid, replace=False, random_state=self.random_state).index.tolist()
            elif (n_valid > valid_train_size) and (n_invalid <= invalid_train_size):
                n_invalid += n_valid - valid_train_size

                valid_idx = df[df.not_o_only].index.tolist()
                invalid_idx = df[~df.not_o_only].sample(n=n_invalid, replace=False, random_state=self.random_state).index.tolist()
            else:
                n_valid += n_invalid - invalid_train_size

                valid_idx = df[df.not_o_only].sample(n=n_valid, replace=False, random_state=self.random_state).index.tolist()
                invalid_idx = df[~df.not_o_only].index.tolist()

            train_idx = valid_idx + invalid_idx
        else:
            train_idx = df.sample(n=size, replace=False, random_state=self.random_state).index

        return train_idx

    # adjust index based on newly labeled data
    def adjust_index(self, keys):
        updated_idx = self.df[self.df["ID"].isin(keys)].index.tolist()
        self.train_idx = list(set(self.train_idx) | set(updated_idx))
        self.pool_idx = list(set(self.pool_idx) - set(updated_idx))

    # helper function
    def get_index_from_name(self, name):
        return self.df[self.df["name"] == name].index.tolist()

    def get_index_from_id(self, keys):
        return self.df[self.df["ID"].isin(keys)].index.tolist()

# TESTING 
# import itertools

# for train_frac, info_frac, train_valid_same_size in itertools.product(np.arange(0.05, 1.05, 0.05),
#                                                                       #np.hstack([np.arange(0.01, 0.11, 0.01), np.arange(0.15, 1.05, 0.05)]),
#                                                                       np.arange(0, 1.1, 0.1),
#                                                                       [True, False]):
#     train_frac = round(train_frac, 2)
#     info_frac = round(info_frac, 1)

#     print(f"FRAC: {train_frac}, INFO FRAC: {info_frac}, SAME SIZE: {train_valid_same_size}")

#     dataset = SimpleNERDataset("memc", train_frac=train_frac, 
#                                        info_frac=info_frac, 
#                                        train_valid_same_size=train_valid_same_size)
#     df = dataset.df
#     try:
#         true_info_prop = df[df.name == "train"].not_o_only.value_counts()[True] / len(df[df.name == "train"])
#     except Exception:
#         true_info_prop = 1 - df[df.name == "train"].not_o_only.value_counts()[False] / len(df[df.name == "train"])

#     print(df.name.value_counts())
#     print(f"TRUE INFO PROP: {true_info_prop}")
