import os
import torch
import pathlib
import itertools

import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F

from tqdm import tqdm

from sklearn.metrics import classification_report

from collections import Counter, defaultdict

from torch.utils.data import DataLoader, TensorDataset

from datasets import concatenate_datasets
from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification
from transformers import DataCollatorForTokenClassification, Trainer, TrainingArguments

from setting import setting
from utils.data import SimpleNERDataset
from utils.bert import create_tokenized_datasets
from utils.common import evaluate_prediction

from scipy import stats

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pad_token_label_id = -100

config = AutoConfig.from_pretrained(setting.config_path)
tokenizer = AutoTokenizer.from_pretrained(setting.tokenizer_path, config=config, add_prefix_space=True)

####################################################################################################
# generic functions

def _euclidean_metric(a, b, normalize=False):
    if normalize:
        a = F.normalize(a)
        b = F.normalize(b)
    n = a.shape[0]
    m = b.shape[0]
    a = a.unsqueeze(1).expand(n, m, -1)
    b = b.unsqueeze(0).expand(n, m, -1)
    logits = -((a - b) ** 2).sum(dim=2)
    return logits


def sample_support_dataset(tokenized_datasets, n_shot=5):
    # sample support datasets from tokenized_dataset["eval"]
    sentence_ids = None
    filter_func = lambda example: ("SN" or "SV") in example["tags"]

    if n_shot is None:
        # support_dataset = tokenized_datasets["eval"]
        # support_dataset = concatenate_datasets([tokenized_datasets[split].filter(filter_func) for split in ["train", "eval"]])
        support_dataset = tokenized_datasets["eval"].filter(filter_func)
        sentence_ids = support_dataset["ID"]
    else:
        # select all samples with SN/SV in it, follow the algorithm 1 in the paper
        # https://arxiv.org/pdf/2010.02405.pdf
        subset = tokenized_datasets["eval"].filter(filter_func)
        # make sure K-shot requirement is met
        total_cnt = Counter(list(itertools.chain(*subset["tags"])))
        if (total_cnt["SN"] < n_shot) or (total_cnt["SV"] < n_shot):
            support_dataset = subset
        else:
            # idea:
            # - obtain the dict of {ID: #SN/SV} 
            # - sort the dict in the ascending order, obtain the cumsum dict {cumsum: [ID1, ID2, ...]}
            # - find the keys corresponding to cumsum, if does not exist, find the closet one
            # sample by SV 
            if total_cnt["SN"] >= total_cnt["SV"]:
                cnt_dict = {sentence_id: Counter(tag_lst)["SV"] for sentence_id, tag_lst in zip(subset["ID"], subset["tags"])}
            # sample by SN
            else:
                cnt_dict = {sentence_id: Counter(tag_lst)["SN"] for sentence_id, tag_lst in zip(subset["ID"], subset["tags"])}
            
            cnt_dict = dict(sorted(cnt_dict.items(), key=lambda x: x[1]))
            key_dict = {key: list(cnt_dict.keys())[:idx+1] for key, idx in zip(np.cumsum(list(cnt_dict.values())), range(len(cnt_dict)))}

            # K might not exist as a key in key_dict, so modify the key_dict and select the smallest one
            key_dict = {key - n_shot: val for key, val in key_dict.items() if key >= n_shot}
            sentence_ids = key_dict[min(key_dict, key=key_dict.get)]
        
            support_dataset = subset.filter(lambda example: example["ID"] in sentence_ids)
        
    print("SUPPORT SET STATISTICS")
    print(f"NUMBER OF SAMPLED SENTENCES: {len(support_dataset)}")
    print(Counter(list(itertools.chain(*support_dataset["tags"]))))
    print("SAMPLED SENTENCES")
    print(sentence_ids)

    return support_dataset


def create_support_dataset(tokenized_datasets, sentence_ids=None):
    # create support set from provided list of IDs
    if isinstance(sentence_ids, list) and sentence_ids:
        support_dataset = tokenized_datasets["eval"].filter(lambda example: example["ID"] in sentence_ids)
    else:
        support_dataset = tokenized_datasets["eval"]
    
    print("SUPPORT SET STATISTICS")
    print(f"NUMBER OF SAMPLED SENTENCES: {len(support_dataset)}")
    print(Counter(list(itertools.chain(*support_dataset["tags"]))))
    
    return support_dataset
    
        
def verify_tokenized_datasets(dataset):
    # dataset: something similar to tokenized_datasets["test"]
    # padding
    # - input_ids, attention_mask, token_type_ids: 0
    # - labels: -100
    if "roberta" in setting.model_name.split("-"):
        column_names = ["input_ids", "attention_mask", "labels"]
    else:
        column_names = ["input_ids", "attention_mask", "labels", "token_type_ids"]

    flag = False
    for name in column_names:
        lst = dataset[name]
        # there should be only one length if all input are correctly padded
        if len(Counter(map(len, lst))) > 1: 
            print("DETECTED UNPADDED INPUT")
            flag = True
            break
        
    if flag:
        padded_dataset = dict()
        for name in column_names:
            lst = dataset[name]
            padded_length = Counter(list(map(len, lst))).most_common(1)[0][0]
            padded_token = -100 if name == "labels" else 0

            padded_dataset[name] = [list(np.pad(sample, pad_width=(0, padded_length-len(sample)), constant_values=(padded_token, padded_token))) 
                                    for sample in lst]
    else: 
        padded_dataset = dataset
    
    return padded_dataset

####################################################################################################
# CRF decoding

def get_abstract_transitions(tokenized_datasets):
    """
    Compute abstract transitions on the training dataset for StructShot
    """

    tag_lists = tokenized_datasets["train"]["tags"]

    s_o, s_i = 0., 0.
    o_o, o_i = 0., 0.
    i_o, i_i, x_y = 0., 0., 0.
    for tags in tag_lists:
        if tags[0] == 'O': s_o += 1
        else: s_i += 1
        for i in range(len(tags)-1):
            p, n = tags[i], tags[i+1]
            if p == 'O':
                if n == 'O': o_o += 1
                else: o_i += 1
            else:
                if n == 'O':
                    i_o += 1
                elif p != n:
                    x_y += 1
                else:
                    i_i += 1

    trans = []
    trans.append(s_o / (s_o + s_i))
    trans.append(s_i / (s_o + s_i))
    trans.append(o_o / (o_o + o_i))
    trans.append(o_i / (o_o + o_i))
    trans.append(i_o / (i_o + i_i + x_y))
    trans.append(i_i / (i_o + i_i + x_y))
    trans.append(x_y / (i_o + i_i + x_y))
    return trans


START_ID = 0
O_ID = 1

class ViterbiDecoder:
    """
    Generalized Viterbi decoding
    """

    def __init__(self, n_tag, abstract_transitions, tau):
        """
        We assume the batch size is 1, so no need to worry about PAD for now
        n_tag: START, O, and I_Xs
        """
        super().__init__()
        self.transitions = self.project_target_transitions(n_tag, abstract_transitions, tau)

    @staticmethod
    def project_target_transitions(n_tag, abstract_transitions, tau):
        s_o, s_i, o_o, o_i, i_o, i_i, x_y = abstract_transitions
        # self transitions for I-X tags
        a = torch.eye(n_tag) * i_i
        # transitions from I-X to I-Y
        b = torch.ones(n_tag, n_tag) * x_y / (n_tag - 3)
        c = torch.eye(n_tag) * x_y / (n_tag - 3)
        transitions = a + b - c
        # transition from START to O
        transitions[START_ID, O_ID] = s_o
        # transitions from START to I-X
        transitions[START_ID, O_ID+1:] = s_i / (n_tag - 2)
        # transition from O to O
        transitions[O_ID, O_ID] = o_o
        # transitions from O to I-X
        transitions[O_ID, O_ID+1:] = o_i / (n_tag - 2)
        # transitions from I-X to O
        transitions[O_ID+1:, O_ID] = i_o
        # no transitions to START
        transitions[:, START_ID] = 0.

        powered = torch.pow(transitions, tau)
        summed = powered.sum(dim=1)

        transitions = powered / summed.view(n_tag, 1)

        transitions = torch.where(transitions > 0, transitions, torch.tensor(.000001))

        #print(transitions)
        #print(torch.sum(transitions, dim=1))
        return torch.log(transitions)

    def forward(self, scores: torch.Tensor) -> torch.Tensor:  # type: ignore
        """
        Take the emission scores calculated by NERModel, and return a tensor of CRF features,
        which is the sum of transition scores and emission scores.
        :param scores: emission scores calculated by NERModel.
            shape: (batch_size, sentence_length, ntags)
        :return: a tensor containing the CRF features whose shape is
            (batch_size, sentence_len, ntags, ntags). F[b, t, i, j] represents
            emission[t, j] + transition[i, j] for the b'th sentence in this batch.
        """
        batch_size, sentence_len, _ = scores.size()

        # expand the transition matrix batch-wise as well as sentence-wise
        transitions = self.transitions.expand(batch_size, sentence_len, -1, -1)

        # add another dimension for the "from" state, then expand to match
        # the dimensions of the expanded transition matrix above
        emissions = scores.unsqueeze(2).expand_as(transitions)

        # add them up
        return transitions + emissions

    @staticmethod
    def viterbi(features: torch.Tensor) -> torch.Tensor:
        """
        Decode the most probable sequence of tags.
        Note that the delta values are calculated in the log space.
        :param features: the feature matrix from the forward method of CRF.
            shaped (batch_size, sentence_len, ntags, ntags)
        :return: a tensor containing the most probable sequences for the batch.
            shaped (batch_size, sentence_len)
        """
        batch_size, sentence_len, ntags, _ = features.size()

        # initialize the deltas
        delta_t = features[:, 0, START_ID, :]
        deltas = [delta_t]

        # use dynamic programming to iteratively calculate the delta values
        for t in range(1, sentence_len):
            f_t = features[:, t]
            delta_t, _ = torch.max(f_t + delta_t.unsqueeze(2).expand_as(f_t), 1)
            deltas.append(delta_t)

        # now iterate backward to figure out the most probable tags
        sequences = [torch.argmax(deltas[-1], 1, keepdim=True)]
        for t in reversed(range(sentence_len - 1)):
            f_prev = features[:, t + 1].gather(
                2, sequences[-1].unsqueeze(2).expand(batch_size, ntags, 1)).squeeze(2)
            sequences.append(torch.argmax(f_prev + deltas[t], 1, keepdim=True))
        sequences.reverse()
        return torch.cat(sequences, dim=1)

####################################################################################################
# StructShot: consider support and test sentences independently

def get_token_encodings_and_labels(model, batch):
    """
    Get token encoding using pretrained BERT-NER model as well as groundtruth label
    """
    model.to(device)
    batch = tuple(t.to(device) for t in batch)

    # label_batch = batch[3]
    label_batch = batch[2]
    with torch.no_grad():
        inputs = {"input_ids": batch[0], "attention_mask": batch[1], "output_hidden_states": True}
        outputs = model(**inputs)
        hidden_states = outputs[1][-1]  # last layer representations
    return hidden_states, label_batch


def get_support_encodings_and_labels(model, support_loader):
    """
    Get token encodings and labels for all tokens in the support set
    """
    support_encodings, support_labels = [], []
    for batch in tqdm(support_loader, desc="Support data representations"):
        # encodings: (batch_size, max_len, 768)
        # labels: (batch_size, max_len)
        encodings, labels = get_token_encodings_and_labels(model, batch)
        encodings = encodings.view(-1, encodings.shape[-1])
        labels = labels.flatten()
        # NOTE: filter out PAD tokens
        idx = torch.where(labels != pad_token_label_id)[0]
        support_encodings.append(encodings[idx])
        support_labels.append(labels[idx])
    return torch.cat(support_encodings), torch.cat(support_labels)


# with CRF
def nn_decode(reps, support_reps, support_tags, n_neighbor=3):
    """
    NNShot: neariest neighbor decoder for few-shot NER
    """
    batch_size, sent_len, ndim = reps.shape
    scores = _euclidean_metric(reps.view(-1, ndim), support_reps, True)

    n, m = scores.shape
    n_tags = torch.max(support_tags) + 1
    emissions = -100000. * torch.ones(n, n_tags, n_neighbor).to(scores.device)
    for t in range(n_tags):
        mask = (support_tags == t).float().view(1, -1)
        masked = scores * mask
        masked = torch.where(masked < 0, masked, torch.tensor(-100000.).to(scores.device))

        for knn in range(n_neighbor):
            max_val, max_index = torch.max(masked, dim=1)
            
            # emissions
            emissions[:, t, knn] = max_val

            # mask larger values to -100000
            masked[max_val.view(-1, 1) == masked] = -100000.

    # tag predictions
    tags = list()
    for i in range(n):
        tag_sim_list = [(j, emissions[i][j][k]) for j in range(n_tags) for k in range(n_neighbor)]
        tag = stats.mode([item[0] for item in sorted(tag_sim_list, key=lambda x: x[1], reverse=True)[:n_neighbor]]).mode[0]
        tags.append(tag)

    # collapse the last axis for emissions
    # return torch.tensor(tags).view(batch_size, sent_len), torch.max(emissions, dim=-1)[0].view(batch_size, sent_len, -1)
    return torch.tensor(tags).view(batch_size, sent_len), torch.mean(emissions, dim=-1).view(batch_size, sent_len, -1)


def get_nn_emissions(scores, tags):
    """
    Obtain emission scores from NNShot
    """
    n, m = scores.shape
    n_tags = torch.max(tags) + 1
    emissions = -100000. * torch.ones(n, n_tags).to(scores.device)
    for t in range(n_tags):
        mask = (tags == t).float().view(1, -1)
        masked = scores * mask
        masked = torch.where(masked < 0, masked, torch.tensor(-100000.).to(scores.device))
        emissions[:, t] = torch.max(masked, dim=1)[0]
    return emissions


def predict_structshot_model(model, support_dataset, test_dataset, abstract_transitions, crf=True, n_neighbor=1):
    # support_dataset= sample_support_dataset(tokenized_datasets, n_shot=n_shot)
    # test_dataset = tokenized_datasets["test"]

    if "roberta" in setting.model_name.split("-"):
        column_names = ["input_ids", "attention_mask", "labels"]
    else:
        column_names = ["input_ids", "attention_mask", "labels", "token_type_ids"]

    # variables specific to structshot
    tau = 0.05
    batch_size = 1

    label_map = {v: k for k, v in setting.label_to_id.items()}
    label_list = setting.label_list

    # NOTE: because of HF's bug, some sentences might not be correctly padded
    #       make sure they are correctly padded before converting them into tensor
    support_dataset = verify_tokenized_datasets(support_dataset)
    test_dataset = verify_tokenized_datasets(test_dataset)

    support_data = [torch.as_tensor(np.array(support_dataset[name]), dtype=torch.long)
                    for name in column_names]
    test_data = [torch.as_tensor(np.array(test_dataset[name]), dtype=torch.long)
                for name in column_names]
    
    support_dataloader = DataLoader(TensorDataset(*support_data), batch_size=batch_size)
    support_encodings, support_labels = get_support_encodings_and_labels(model, support_dataloader)

    test_dataloader = DataLoader(TensorDataset(*test_data), batch_size=batch_size)

    # 2. NNShot
    preds = None
    emissions = None
    out_label_ids = None
    for batch in tqdm(test_dataloader, desc="Test data representations"):
        encodings, labels = get_token_encodings_and_labels(model, batch)
        nn_preds, nn_emissions = nn_decode(encodings, support_encodings, support_labels, n_neighbor=n_neighbor)
        if preds is None:
            preds = nn_preds.detach().cpu().numpy()
            emissions = nn_emissions.detach().cpu().numpy()
            out_label_ids = labels.detach().cpu().numpy()
        else:
            preds = np.append(preds, nn_preds.detach().cpu().numpy(), axis=0)
            emissions = np.append(emissions, nn_emissions.detach().cpu().numpy(), axis=0)
            out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)


    out_label_list = [[] for _ in range(out_label_ids.shape[0])]
    emissions_list = [[] for _ in range(out_label_ids.shape[0])]
    preds_list = [[] for _ in range(out_label_ids.shape[0])]

    for i in range(out_label_ids.shape[0]):
        for j in range(out_label_ids.shape[1]):
            if out_label_ids[i, j] != pad_token_label_id:
                out_label_list[i].append(label_map[out_label_ids[i][j]])
                emissions_list[i].append(emissions[i][j])
                preds_list[i].append(label_map[preds[i][j]])

    # 3. StructShot (Viterbi decoding)
    if crf:
        print("USING CRF DECODING")
        viterbi_decoder = ViterbiDecoder(len(label_list)+1, 
                                         abstract_transitions, 
                                         tau)
        preds_list = [[] for _ in range(out_label_ids.shape[0])]
        for i in range(out_label_ids.shape[0]):
            sent_scores = torch.tensor(emissions_list[i])
            sent_len, n_label = sent_scores.shape
            sent_probs = F.softmax(sent_scores, dim=1)
            start_probs = torch.zeros(sent_len) + 1e-6
            sent_probs = torch.cat((start_probs.view(sent_len, 1), sent_probs), 1)
            feats = viterbi_decoder.forward(torch.log(sent_probs).view(1, sent_len, n_label+1))
            vit_labels = viterbi_decoder.viterbi(feats)
            vit_labels = vit_labels.view(sent_len)
            vit_labels = vit_labels.detach().cpu().numpy()
            for label in vit_labels:
                preds_list[i].append(label_map[label-1])
    
    return out_label_list, preds_list


def evaluate_structshot_model(model, tokenized_datasets, crf=True, n_shot=None, n_neighbor=3):
    support_dataset = sample_support_dataset(tokenized_datasets, n_shot=n_shot)
    test_dataset = tokenized_datasets["test"]

    y_true, y_pred = predict_structshot_model(model, support_dataset, test_dataset, crf=True, n_shot=None, n_neighbor=n_neighbor)
    metric, metric_str = evaluate_prediction(y_true, y_pred)

    return metric, metric_str
